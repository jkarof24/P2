This project has demonstrated how data that violates the assumptions of multicollinearity and heteroscedasticity in ordinary least squares (OLS) regression models can be identified. The effect of violating the assumption of homoscedasticity has been illustrated using synthetic data.
\\
It is important to note that in real-world scenarios, the violation of the homoscedasticity assumption is usually not the only issue. It has been shown that one way to build accurate regression models in the presence of such assumption violations is through bootstrapping.
\\
There are also alternative methods, such as weighted least squares (WLS), which assigns different weights to data points depending on the variance at each point. Another option is to transform the data using techniques like log transformation or square root transformation. Performing OLS regression on the transformed data can sometimes yield a more accurate model.
\\
However, it is important to understand that, like bootstrapping, these alternatives also have drawbacks. In the case of WLS, the weights for each data point are unknown and estimating them can be computationally expensive and introduce uncertainty into the model. It also makes interpretation more difficult, as each data point is adjusted by an unknown weight.
\\
Log and square root transformations are simpler, but they do not always resolve assumption violations. In some cases, a log transformation may even worsen heteroscedasticity. Additionally, interpreting models based on transformed data becomes more complex, as the relationships are no longer between the original values but between their logarithms or square roots.
\\
This highlights why bootstrapping remains widely used despite its computational burden. Because it relies on the empirical distribution of the data, it requires fewer assumptions than OLS and produces results that are easier to interpret. However, bootstrapping also has limitations, such as deciding how many resamples to use and, in the case of parametric bootstrapping, assuming that the sample distribution accurately represents the population. Furthermore, estimating parameters using the mean can make the model vulnerable to outliers.
