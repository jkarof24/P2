%This project has demonstrated how data that violates the assumptions of multicollinearity and heteroscedasticity in ordinary least squares (OLS) regression models can be identified. The effect of violating the assumption of homoscedasticity has been illustrated using synthetic data.
%It is important to note that in real-world scenarios, the violation of the homoscedasticity assumption is usually not the only issue. It has been shown that one way to build accurate regression models in the presence of such assumption violations is through bootstrapping.
%There are also alternative methods, such as weighted least squares (WLS), which assigns different weights to data points depending on the variance at each point. Another option is to transform the data using techniques like log transformation or square root transformation. Performing OLS regression on the transformed data can sometimes yield a more accurate model.
%However, it is important to understand that, like bootstrapping, these alternatives also have drawbacks. In the case of WLS, the weights for each data point are unknown and estimating them can be computationally expensive and introduce uncertainty into the model. It also makes interpretation more difficult, as each data point is adjusted by an unknown weight.
%Log and square root transformations are simpler, but they do not always resolve assumption violations. In some cases, a log transformation may even worsen heteroscedasticity. Additionally, interpreting models based on transformed data becomes more complex, as the relationships are no longer between the original values but between their logarithms or square roots.
%This highlights why bootstrapping remains widely used despite its computational burden. Because it relies on the empirical distribution of the data, it requires fewer assumptions than OLS and produces results that are easier to interpret. However, bootstrapping also has limitations, such as deciding how many resamples to use and, in the case of parametric bootstrapping, assuming that the sample distribution accurately represents the population. Furthermore, estimating parameters using the mean can make the model vulnerable to outliers.

\subsection{Assumption violations and implications}
The objective of this prroject has been to investigate how breaking key asssumptions, like homoskcedasticity and multicollinarity, impacts performance of classical polynomial regression models, and how Monte Carlo bootstrapping can offer a more reliable alternative. Synthetic data were used to deliberately violate the assumption of homoscedasticity while controlling other variables. This allowed for a focused analysis of how non-constant error variance influences model accuracy and inference.

Violating the homoscedasticity assumption was shown to significantly distort the performance of the ordinary least squares (OLS) model. The resulting regression had a notably high Root Mean Square Error (RMSE) and a strong negative bias, indicating that such violations can invalidate the statistical conclusions. 

\subsection{Bootstrapping as a Solution}
To address these assumption violations, bootstrapping was applied as a nonparametric resampling method. Since it is based on the empirical distribution of the data rather than relying on theoretical assumptions, bootstrapping offers a flexible alternative to OLS.

The results support this effectiveness. The bootstrap model produced a substantially lower RMSE (1377 vs. 1921) and a near-zero MBE (22 vs. -539), indicating more accurate and less biased predictions. This suggests that bootstrapping can successfully mitigate the effects of assumption violations and improve model robustness.

\subsection{Alternative approaches and their limitations}
Several alternative methods can be used to handle heteroscedasticity. One is Weighted Least Squares (WLS), which adjusts for variable variance by assigning different weights to each data point. However, determining appropriate weights is often computationally intensive and can introduce more uncertainty, since estimates weights are used. Additionally, it complicates model interpretation, as results depend on the weighting scheme.

Transforming the data with techniques like log or square root transformations, is another common approach. While these can sometimes stabilize variance, they do not consistently resolve assumption violations. In some cases, they may even worsen heteroscedasticity. Again, interpretation becomes more difficult, as the model no longer operates on the original scale of the variables.


\subsection{Strengths and limits of bootstrapping}
Because of those reasons, bootstrapping remains widely used despite its computational demands. Because it draws directly from the empirical distribution, bootstrapping makes fewer theoretical assumptions and often yields more robust results when an assumption like homoscedasticity is violated. However, it is not without limitations. The number of resamples must be chosen carefully to balance precision and efficiency. In the case of parametric bootstrapping, the method still relies on the assumption that the sample accurately represents the population. Another limitation comes from the use of the mean to estimate parameters, which can make bootstrap models sensitive to outliers and lead to distorted results if such anomalies are not properly addressed.