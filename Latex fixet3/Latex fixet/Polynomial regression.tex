\subsection{Regression Model}
Understanding the relationship between variables is crucial when working with data, as it forms the basis for drawing meaningful conclusions. Regression models are a fundamental statistical tool used to model and analyse these relationships. In this project the chosen model, we are working with is polynomial regression. This section will explain the neccessary theory to understand this model. It will also explain assumption violations in more detail. Specifically the assumption of constant variance of errors also called homoscedasticity and the assumption of no multicollinearity.

\subsubsection{Regression}
Sections $4.6.1$, $4.6.2$ and $4.6.3$ are based on the book 'Applied Lienar Statistical Models' \cite{AppliedLSM}. 
\newline 

\noindent Before looking at polynomial regression, we have to understand linear regression. \newline 
Linear regression is a model that estimates the relationship between a dependent variable, \( Y \), and one or more independent variables, \( x \). A reasonable relationship between the two in simple regression is the linear relationship:
\begin{equation}
Y = \beta_0 + \beta_1 x .
\end{equation}


\noindent Where \( \beta_0 \) is the intercept, and \( \beta_1 \) is the slope.

\noindent In a lot of cases, there will be more independent variables, so the relationship for multiple regression will look like this:

\begin{equation}
	Y = \beta_0 + \beta_1 x_1 + ......+ \beta_n x_n .
\end{equation}




\noindent Where \( n \) is the number of independent variables, and $\beta_2$ further shapes the curvature and complexity of the curve. A widely used method to determine the parameters of linear models is the least square method, in order to find the best fitting line for the data.

\noindent In simple linear regression, the random error \( \epsilon \) is included:
\begin{equation}
Y = \beta_0 + \beta_1 x + \epsilon .
\end{equation}


\noindent It is assumed that \( \epsilon \) is distributed with $E(\epsilon) = 0$ and $Var(\epsilon) = \sigma^2$, and it has consistent variance, which is usually called the Homogeneous Variance Assumption. The random error, \( \epsilon \), adds randomness to account for the natural variability in real data, making the model more realistic.
\newline\\
Polynomial regression is a form of linear regression, but the relationship between \( x \) and \( Y \) is an \( n \)th-degree polynomial. It fits a nonlinear relationship between the value of \( x \) and the corresponding conditional mean of \( Y \), meaning the model predicts the expected value of \( Y \) given \( x \). \newline

\noindent That is why it is used when the relationship between the independent variable and the dependent variable is better represented by a curve rather than a straight line, since it can show the nonlinear patterns in the data.
In polynomial regression, as mentioned earlier, there are six assumptions, that should be met, for the model to be as accurate as possible.


\subsubsection{The Method of Least Squares}

\noindent To find connections in data, it is necessary to estimate coefficients, $\beta_0$ and $\beta_1$, in linear models. 
A widely used method to estimate the coefficients, is the previously mentioned least squares method, which will be referred to as Ordinary Least Squares or OLS in the rest of the project. \newline

\noindent OLS considers the deviation of $Y_i$ for  its expected value, where the observations are $(X_i, Y_i)$. 
This method also requires, that we consider the sum of the $n$ squared deviations.
This is denoted by the criterion $Q$,
\begin{equation}
Q=\sum_{i=1}^{n}(Y_i-\beta_0 - \beta_1 X_i)^2 .
\end{equation}

\noindent According to the method of OLS, the estimations of $\beta_0$ and $\beta_1$ that minimize Q for the given sample observations $(X_1,Y_1), (X_2,Y_2), ..., (X_n,Y_n)$, are called $b_0$ and $b_1$.  

\noindent If an analytical approach is used, the values $b_0$ and $b_1$ that minimize Q for any particular set of sample data are given by these simultaneous equations: 
\begin{equation}
\sum Y_i =n b_0 +b_1 \sum X_i ,
\end{equation}

\begin{equation}
\sum X_i Y_i = b_0 \sum X_i + b_1 \sum X_1^2 .
\end{equation}

\noindent These equations are called $normal equations$, where $b_0$ and $b_1$ are the $point estimators$ of $\beta_0$ and $\beta_1$. It is possible to calculate these normal equations simultaneously for $b_0$ and $b_1$ through these expressions,

\begin{equation}
b_1 = \frac{\sum (X_1 - \bar{X}) (Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2} ,
\end{equation}

\begin{equation}
	b_0 = \frac{1}{n} (\sum Y_i - b_1 \sum X_i ) = \bar{Y} - b_1 \bar{X} .
\end{equation}

\noindent Here, $\bar{X}$ and $\bar{Y}$ are the means of $X_i$ and $Y_i$. The normal equations can also be derived by differentiating with respect to $\beta_0$ and $\beta_1$:

\begin{equation}
	\frac{\partial Q}{\partial \beta_0}=-2 \sum (Y_i - \beta_0 - \beta_1 X_i) ,
\end{equation}
\begin{equation}
\frac{\partial Q} {\partial \beta_1} = -2 \sum X_i (Y_i - \beta_0 - \beta_1 X_i) .
\end{equation}

\noindent By setting these derivatives equal to zero, we can find $b_0$ and $b_1$, 

\begin{equation}
	-2 \sum (Y_i - b_0 - b_1 X_1)=0 ,
\end{equation}
\begin{equation}
	-2\sum X_i(Y_i - b_0 - b_1 X_1)=0 .
\end{equation}

\noindent This can be simplified, 
\begin{equation}
\sum_{i=1}^{n} (Y_i - b_0 - b_1 Xi)=0£ ,
\end{equation}
\begin{equation}
	\sum_{i=1}^{n} X_1(Y_i - b_0 - b_1 Xi)=0£ .
\end{equation}
And it can be expanded, so the normal equations are obtained, 

\begin{equation}
	\sum Y_1 - n b_0 - b_1 \sum X_1 =0 ,
\end{equation}

\begin{equation}
	\sum X_1 Y_1  - b_0 \sum X_1 - b_1 \sum X_i^2 =0 .
\end{equation}

\noindent Solving this for $b_0$ and $b_1$ will lead to values, that minimize Q, and these are the estimates for $\beta_0$ and $\beta_1$. 
When rearranging terms, we get the normal equations $(28)$ and $(29)$. The estimates $b_0$ and $b_1$ obtain the minimum when checking the second partial derivatives. \newline


\subsubsection{Linear Models}
Equations for linear models can be written in matrix terms, where the normal error regression model for simple linear regression is.
 

\begin{equation}
Y_i = \beta_0 + \beta_1 X_i 0 \epsilon_i .
\end{equation}


\noindent Which implies that,

\begin{equation}
Y_1 = \beta_0 + \beta_1 X_1 + \epsilon_1
\end{equation}

\begin{equation}
Y_2 = \beta_0 + \beta_1 X_2 + \epsilon_2
\end{equation}
$$\vdots$$
\begin{equation}
Y_n = \beta_0 + \beta_1 X_n + \epsilon_n .
\end{equation}

\noindent The observations vector $Y$ is,
\begin{equation} Y_{n \times 1} =
\left[
\begin{array}{c}
	Y_1 \\ 
	Y_2 \\ 
	\vdots \\
	Y_n 
\end{array}
\right].
\end{equation}

\noindent The X matrix is, 

\begin{equation}X_{n \times 2}=
\left[
\begin{array}{cc}
	1 & X_1 \\ 
	1 & X_2 \\ 
	\vdots & \vdots \\
	1 & X_n
\end{array}
\right].
\end{equation}


\noindent The $\beta$ vector is, 
\begin{equation}\beta_{2 \times 1} =
\left[
\begin{array}{c}
	\beta_0 \\ 
	\beta_1 
\end{array}
\right].
\end{equation}

\noindent And the $\epsilon$ vector is,
\begin{equation} \epsilon_{n \times 1} =
\left[
\begin{array}{c}
	\epsilon_0 \\ 
	\epsilon_1 \\
	\vdots \\
	\epsilon_n 
\end{array}
\right].
\end{equation}

\noindent This can be written in matrix terms with a dot product, 
\begin{equation} Y_{n \times 1}=X_{n \times 2} \cdot \beta_{2 \times 1} + \epsilon_{n \times 1}.
\end{equation}

\noindent Here
\textbf{$Y$} is a vector of response,
\textbf{$\beta$} is a vector of parameters,
\textbf{$X$} is a matrix of constants, called det design matrix, and
\textbf{$\epsilon$} is a vector of independent normal random variables with expectation.\\

\noindent This can be shown in columns,
\begin{equation}
\left[
\begin{array}{c}
	Y_1 \\ 
	Y_2 \\ 
	\vdots \\
	Y_n 
\end{array}
\right]
=
\left[
\begin{array}{cc}
	1 & X_1 \\ 
	1 & X_2 \\ 
	\vdots & \vdots \\
	1 & X_n
\end{array}
\right]
\left[
\begin{array}{c}
	\beta_0 \\ 
	\beta_1 
\end{array}
\right]
+
\left[
\begin{array}{c}
	\epsilon_0 \\ 
	\epsilon_1 \\
	\vdots \\
	\epsilon_n 
\end{array}
\right].
\end{equation}

\noindent If the dependent variable $Y$ has more than one independent variable in a linear model, the equation looks like this, 

\begin{equation} Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p-1 X_{i, p-1} + \epsilon_i .
\end{equation}

\noindent In matrix terms that is,  
\begin{equation}
\left[
\begin{array}{c}
	Y_1 \\ 
	Y_2 \\ 
	\vdots \\
	Y_n 
\end{array}
\right]
=
\left[
\begin{array}{cccc}
	1 & X_{11} & ... & X_{1, p-1} \\ 
	1 & X_{21} & ... & X_{2, p-1} \\ 
	\vdots & \vdots &  & \vdots \\
	1 & X_{n1} & ... & X_{n, p-1}
\end{array}
\right]
\left[
\begin{array}{c}
	\beta_0 \\ 
	\beta_1 \\
	\vdots \\
	\beta_p-1 
\end{array}
\right]
+
\left[
\begin{array}{c}
	\epsilon_0 \\ 
	\epsilon_1 \\
	\vdots \\
	\epsilon_n 
\end{array}
\right].
\end{equation}

\noindent The $Y$ and $\epsilon$ vectors are the same as in the simple linear regression matrix. The $\beta$ vector has additional parameters, and the $X$ matrix now has a column of $n$ observations for each $p-1 X$ variables. 

\subsubsection{Polynomial Regression}
Now we understand the basic relationships of regression models. Now, it is time for the model , we will be working with in this project, which is polynomial regression. In polynomial regression models the relationship is like this, 
\begin{equation}
	Y=\beta_0 + \beta_1 x + \beta_2 x^2	+ ... + \beta_{p-1} x^{p-1}+ \epsilon .
	\end{equation}

\noindent The coefficients can still be found through the OLS method,

\begin{equation}
	Q=\sum_{i=1}^{n}(Y_i -(\beta_0 + \beta_1 X_i + \beta_2 x_i^2 + ... + \beta_{p-1}X_{i}^{p-1}))^2 .
\end{equation}
\newline

\noindent This can also be written in matrix terms as,
\begin{equation}
	Q=(Y-X\beta)' (Y-X\beta). 
\end{equation}

\noindent Where the design matrix for polynomial regression is, 

\begin{equation}
	 X=
\left[
\begin{array}{ccccc}
	1&x_1&x_1^2&...&x_1^{p-1}\\ 
	1&x_2&x_2^2&...&x_2^{p-1} \\
	\vdots & \vdots &\vdots &&\vdots\\
	1&x_n&x_n^2&...&x_n^{p-1} 
\end{array}
\right].
\end{equation}

\noindent We can expand the expression from before, so it looks like this,
\begin{equation}
	Q=Y' Y -Y' X \beta -\beta' X' Y + \beta' X' X \beta .
\end{equation}


\noindent It is possible to find the value of $\beta$ that minimizes Q by differentiating with respect to $\beta_0$ and $\beta_1$,

\begin{equation}
\frac{\partial}{\partial \beta}(Q)=
\left[
\begin{array}{c}
	\frac{\partial Q}{\partial \beta_0}\\ 
	\frac{\partial Q}{\partial \beta_1}
\end{array}
\right].
\end{equation}

\begin{equation}
\frac{\partial}{\partial \beta}(Q)=-2 X' Y + 2X' X \beta .
\end{equation}

\noindent Then the minimum is found by calculating, where the gradient is 0.
\begin{equation}\b-2 X' Y+ 2X' X \beta =0,\end{equation}
\begin{equation} 2X' X \beta = 2X' Y,\end{equation}
\begin{equation}(X' X)^{-1} X' X \beta = (X' X)^{-1} X' Y,\end{equation}
\begin{equation} \beta=(X' X)^{-1} X' Y.\end{equation}
  For this to be calculated, the matrix $X' X$ has to be invertible, so the columns of $X$ have to be linearly independant.
 
 \noindent So, 
 
\begin{equation}
 	X' Xa  =0,
\end{equation}
\begin{equation}
 	a' X' X a =0,
\end{equation}
\begin{equation}
 	(Xa)'(Xa)=0.
\end{equation}
 
 \noindent This shows, that $X'Xa=0$ if and only if $(Xa)'(Xa)=0$, so $Xa=0$. 
 
 \noindent If it is supposed, that there exists a non-trivial solution for $Xa=0$, there also exists a non-trivial solution for $X'Xa=0$. It is therefore only invertible when the null space of $X$ is 0, or if the columns are linearly independent. 
 
\noindent To ensure that the solution gives a minimum, the hessian matrix is calculated,
\begin{equation}
 \frac{\partial^2 Q}{\partial \beta' \beta}=2X'X.
\end{equation} 
 \noindent If it is positive definite, the function Q has a global minimum, which can be shown with,
\begin{equation}
 a' (X' X)a = (Xa)' (Xa).
\end{equation}  
 \noindent As shown in $(63)$,
 these can only be calculated coefficients, if $Xa \not= 0$. This means, that $X'X$ is positive definite in all relevant situations, which is when the coefficients can be calculated. 
 
 \noindent The solution to the OLS method results in only containing $X'X$ and $X'Y$.
 The matrices are,
 
 
\begin{equation}
 X=
 \left[
 \begin{array}{ccc}
 	1 &X_1	& X_1^2\\ 
 	1 & X_2 & X_2^2	\\
 	\vdots & \vdots & \vdots \\
 	1 & X_n& X_n^2
 \end{array}
 \right],
\end{equation}
 
 \begin{equation}
 	X'X=
 \left[
 \begin{array}{ccccc}
 	n &\sum X_i	& \sum X_i^2 & ... &\sum X_i^{p-1}\\ 
 	\sum X_i & \sum X_i^2 & \sum X_i^3&...& \sum X_i^p	\\
 	\sum X_i^2 & \sum X_i^3 & \sum X_i^4 & ... & \sum X_i^{p+1}\\
 	\vdots&\vdots&\vdots& & \vdots	\\
 	\sum X_i^{p-1} &\sum X_i^p& \sum X_i^{p+1} & ... & \sum X_i^{2p-2}
 \end{array}
 \right],
\end{equation}
 
 \begin{equation}
 	X' Y=
 \left[
 \begin{array}{c}
 	\sum Y_i\\ 
 	\sum X_i Y_i	\\
 	\sum X_i^2 Y_i \\
 	\vdots	\\
 	\sum X_i^{p-1} Y_i
 \end{array}
 \right].
\end{equation}
 
 \noindent These are the necessary matrices to use OLS.

	\subsubsection{Assumptions}
	\input{Assumptions}