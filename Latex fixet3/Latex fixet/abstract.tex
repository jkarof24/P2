Regression is a fundamental statistical method used to model and analyse the relationships between variables. Regression models are powerful tools for drawing statistical inferences from data. Despite their power, these models are also quite fragile, if specific assumptions are not met. The objective of this project is exploring what happens if these assumptions are not met, and how to bypass the problems. One assumption is homoscedasticity, which ensures that the variance of the errors are constant. When this assumption does not hold, standard errors, confidence intervals, and model fit metrics such as $R^2$ become unreliable, potentially leading to misleading conclusions. In this project, we investigate whether Monte Carlo Bootstrapping can help improve regression models when a key assumption, like homoscedasticity,is violated. By applying bootstrapping to synthetic data designed to break this assumption, we explore its potential to reduce model bias and improve predictive accuracy. The results show that bootstrapping consistently improves the robustness and reliability of regression models under these conditions, suggesting it is a valuable technique for handling  violation of homoscedasticity.