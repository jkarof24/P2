Regression is a fundamental statistical method used to model and analyze the relationships between variables. Regression models are powerful tools for drawing statistical inferences from data. They serve as a gateway to more advanced statistical analysis and inference. Despite their power, these models are also quite fragile, if specific assumptions are not met.

Th objective of this project we will focus on what happens if these assumptions are not met, and how to bypass the problems.

One assumption is homoscedasticity, which ensures that the variance of the errors are constant. If this assumption is violated, the standard errors for the coefficients and the $R^2$-metric becomes unreliable, but different methods can be used to circumvent this. 


In this project, we investigate whether Monte Carlo Bootstrapping can help improve regression models when key assumptions, specifically homoscedasticity, are violated. By applying bootstrapping to synthetic data designed to break this assumption, we explore its potential to reduce model bias and improve predictive accuracy.
  The results show that bootstrapping consistently improves the robustness and reliability of regression models under these conditions, suggesting it is a valuable technique for handling  violation of homoscedasticity.