\documentclass{article}
\title{Monte Carlo simulation}
\author{Christian, Jonathan, Marcus, Puk & Sofia }
\date{\today}
\begin{document}
	\maketitle
	\newpage
	\tableofcontents
	\newpage
	\section{Introduction}
	\newpage
	\section{Statistical theory}
	\subsection{Probability space}
 
	The \textbf{sample space}, \textit{S}, is the set of all possible outcomes.
	\newline
	If a sample space contains a finite number of possibilities or an unending sequence with as many elements as there are whole numbers, it is called a \textbf{discrete sample space}.
	\newline
Example: When rolling a standard six-sided die form the discrete sample space, the possible outcomes are $S={1,2,3,4,5,6}$ 
 \newline

	If a sample space contains an infinite number of possibilities equal to the number of points on a line segment, it is called a \textbf{continuous sample space}.
	\break
Example: Measuring the heights of people in a population. This is a continuous sample space, because height can take any real value within a given range. 
	\newline
	An \textbf{event} is a subset, $A\subseteq S$, of the sample space. The event is the amount that contains all possible events.
	An example of a discrete event could be rolling a die and getting an uneven number, this would be the event $A={1,3,5}$.
	\newline 
For the continuous event, it could be that a person is between 160 cm and 170 cm tall.
\newline
	
    	The probability of an event \textit{A}, $P(A)$, is the sum of the weights of all sample points in \textit{A}.
	The probability of the whole sample space is 1, $P(S)=1$
	The  probability of any event being between 0 and 1,$0<P(A)<1$
	The probability of the empty set being 0, $P(Ø)=0$
	\newline
 	\newline
 	Probability of mutually exclusive events
  	\newline
	If \textit{A} and \textit{B} are mutually exclusive, $A \cap B=Ø$, then
 	\newline
	$P(A \cup B) = P(A)+P(B)$
 	\newline
  	\newline
	Where \textit{A} and \textit{B} never occur at the same time, so their union is equal to the two events added together. 
	\newline
	Probability of union
	\newline
	$P(A \cup B)=P(A)+P(B)-P(A \cap B)
	 \newline
	Here, the union of the two events is \textit{A} added to \textit{B}, but minus their common event, since it otherwise would be added twice. 
 	\newline
	Two events \textit{A} and \textit{B} are independent, if 
 	\newline
	$P(A|B)=P(A)$
 	\newline
	The equivalent definition to this is:
 	\newline
	Two events \textit{A} and \textit{B} are independent if and only if 
	\newline
	$P(A \cap B)=P(A)P(B)$
 	This says that the probability of both event \textit{A} and \textit{B} happening, is equal to the product of the two events.


	\subsection{Random variable}
	\subsection{Estimators and estimates}
 	If we are interested in certain parameters of a population distribution, we can look at a sample. From this, we can make a \textbf{point estimate}. 
	\newline
	Examples of this are, 
	\newline
	$\bar{x}$ is a point estimate of $\mu$
	\newline
	s is a point estimate of $\sigma$
	\newline
	
	This is often supplemented with a \textbf{confidence interval}
	\newline
	This is an interval around the point estimate, where we are confident that the population parameter is located.
	\newline
	
	For $\mu$, we have different ways of estimating it. We can use the sample mean $\bar{X}$, or the average $X_T$ of the sample upper and lower quartiles. 
	But in this case, we have to look out for \textbf{bias}. If the distribution of a population is skewed, then $X_T$ is biased. The result of this is, that in the long run, this estimator will systematically over or under estimate the value of $\mu$. This is written as,
	\newline
	$E(X_T) \neq \mu$.
	\newline
	It is generally preferred that the estimator is \textbf{unbiased}. In this case, $\bar{X}$ is an unbiased estimate of the population mean $\mu$.
	\newline
	
	The standard error of $\bar{X}$ is $\frac{\sigma}{\sqrt{n}}$. Here, the standard error decreases, when the sample size increases. If an estimator has this property, it is called \textbf{consistent}. If we compare, the estimator $X_T$ is also consistent, but has a greater variance than $\bar{X}$. 
	\newline
	It is generally preferred that the estimator has the smallest possible variance, and in that case it is called efficient. So $\bar{X}$ is an efficient estimator.
	\newline
	When estimating a parameter, the symbol $\hat$ is used above it. For $\mu$, $\hat{\mu} = \bar{X}$ .
	\newline
	We can calculate $\bar{X}$ using the following formula,
	$$\bar{X}=\frac{1}{n} \sum_{i=1}^{n}X_i$$   
\newline
For the variance $\sigma$, we can estimate it by using the formula for $S^2$,
$$S^2=\frac{1}{n-1} \sum_{i=1}(X_i-\bar{X})$$

	\subsection{Probability distribution}
	\subsection{Statistical methods}
	\newpage
	\section{Polynomial regression}
 	
	\subsection{Assumptions}
	\newpage
	\section{Pseudo random number generator}
	\newpage
	\section{Monte Carlo}
	\subsection{Assumptions}
	\newpage
	\section{Problem statement}
	\newpage
	\section{Classical regression}
 	
	\newpage
	\section{Monte Carlo regression}
	\newpage
	\section{Comparison between regressions}
	\newpage
	\section{Discussion}
	\newpage
	\section{Conclusion}
	\newpage
 	\section{Litteratur}
  
\end{document}
