

To understand how assumption violations affect classical polynomial regression, we begin by generating synthetic data using a random number generator. First, four independent variables are created, each normally distributed with known standard deviations. Then, the dependent variable is generated as a function of these four independent variables. This synthetic dataset initially satisfies all the assumptions required for ordinary least squares (OLS) regression.
\\
Next, an error term is added that scales with the dependent variable. This introduces heteroscedasticity meaning the error variance is no longer constant thereby violating the assumption of homoscedasticity. This step is done deliberately to ensure that this is the only assumption being violated, so any observed effects on the model can be attributed specifically to this violation.
\\
The dataset is then split into training and test sets, with $80\%$ used for training and the remaining $20\%$ for testing. A standard linear polynomial regression model is fitted to the training data. The mean bias error, root mean squared error (RMSE), standard error, and confidence intervals are calculated to evaluate the model's performance.
\\
Following this, a bootstrap procedure is performed by resampling the training data 10,000 times. For each resampled dataset, a linear polynomial model is fitted. The coefficients from each model are stored in a new data frame, and predictions are made on the test data and also stored in the dataframe. These predictions are then compared to the actual test values to compute the mean bias error, RMSE, standard error, and confidence intervals for the bootstrap models.




\begin{table}
	\centering
	\caption{Sammenligning af OLS og Bootstrap modeller}
	\begin{tabular}{lcc}
		\hline
		& \textbf{OLS} & \textbf{Bootstrap} \\
		\hline
		\textbf{MBE} & -70.8071 & -40.4049 \\
		\textbf{RMSE} & 253.4575 & 255.4629 \\
		\hline
		\textbf{Std. Error}  \\
		\hline
		Intercept & 7.012772e+01 & 8.390024e+01 \\
		$I(x1^2)$ & 4.362507e-03 & 7.045369e-03 \\
		$I(x2^3)$ & 4.257801e-06 & 2.385538e-05 \\
		$I(x3^4)$ & 1.155885e-09 & 3.319540e-07 \\
		$I(x4^5)$ & 3.274193e-10 & 3.954864e-09 \\
		\hline
		\textbf{Koefficient}  \\
		\hline
		Intercept & [814.5374 , 70.1277] & [599.9494, 932.9073] \\
		$I(x1^2)$ & [0.0107 , 0.0044] & [0.0027, 0.0287] \\
		$I(x2^3)$ & [5.65e-06 , 4.26e-06] & [-1.60e-05, 6.18e-05] \\
		$I(x3^4)$ & [8.16e-09 , 1.16e-09] & [7.24e-09, 4.35e-07] \\
		$I(x4^5)$ & [-2.56e-10 , 3.27e-10] & [-7.14e-09, 7.84e-09] \\
		\hline
	\end{tabular}
\end{table}