%This project has demonstrated how data that violates the assumptions of multicollinearity and heteroscedasticity in ordinary least squares (OLS) regression models can be identified. The effect of violating the assumption of homoscedasticity has been illustrated using synthetic data.
%It is important to note that in real-world scenarios, the violation of the homoscedasticity assumption is usually not the only issue. It has been shown that one way to build accurate regression models in the presence of such assumption violations is through bootstrapping.
%There are also alternative methods, such as weighted least squares (WLS), which assigns different weights to data points depending on the variance at each point. Another option is to transform the data using techniques like log transformation or square root transformation. Performing OLS regression on the transformed data can sometimes yield a more accurate model.
%However, it is important to understand that, like bootstrapping, these alternatives also have drawbacks. In the case of WLS, the weights for each data point are unknown and estimating them can be computationally expensive and introduce uncertainty into the model. It also makes interpretation more difficult, as each data point is adjusted by an unknown weight.
%Log and square root transformations are simpler, but they do not always resolve assumption violations. In some cases, a log transformation may even worsen heteroscedasticity. Additionally, interpreting models based on transformed data becomes more complex, as the relationships are no longer between the original values but between their logarithms or square roots.
%This highlights why bootstrapping remains widely used despite its computational burden. Because it relies on the empirical distribution of the data, it requires fewer assumptions than OLS and produces results that are easier to interpret. However, bootstrapping also has limitations, such as deciding how many resamples to use and, in the case of parametric bootstrapping, assuming that the sample distribution accurately represents the population. Furthermore, estimating parameters using the mean can make the model vulnerable to outliers.

\subsection{Assumption violations and implications}
This project has demonstrated how data that violates the assumptions of multicollinearity and heteroscedasticity in ordinary least squares (OLS) regression models can be identified. The effect of violating the assumption of homoscedasticity has been illustrated using synthetic data. It is important to note that in real-world scenarios, the violation of the homoscedasticity assumption is usually not the only issue. Nevertheless, isolating this assumption in a controlled setting allows for clearer analysis of its individual impact on model performance. %Although multicollinearity was not the primary focus of the regression comparison, its presence was considered in the overall analysis of assumption violations.

\subsection{Bootstrapping as a Solution}
The analysis shows that one way to build accurate regression models in the presence of assumption violations is through bootstrapping. Because it relies on the empirical distribution of the data, bootstrapping requires fewer assumptions than OLS and produces results that are easier to interpret. The bootstrap model exhibited improved performance, with a significantly lower Root RMSE of 1377 compared to 1921 for the OLS model, and a MBE of 22, compared to -539 for OLS. These results indicate better predictive accuracy and reduced bias.

\subsection{Alternative approaches and their limitations}
There are also alternative methods, such as Weighted Least Squares (WLS), which assign different weights to data points depending on the variance at each point. Another option is to transform the data using techniques like log transformation or square root transformation. Performing OLS regression on the transformed data can sometimes yield a more accurate model. However, it is important to understand that, like bootstrapping, these alternatives also have drawbacks. In the case of WLS, the weights for each data point are unknown and estimating them can be computationally expensive and introduce uncertainty into the model. It also complicates interpretation, as each data point is adjusted by an unknown weight. Log and square root transformations are simpler but do not always resolve assumption violations. In some cases, a log transformation may even worsen heteroscedasticity. Additionally, interpreting models based on transformed data becomes more complex, as the relationships are no longer between the original values but between their logarithms or square roots.

\subsection{Strengths and limits of bootstrapping}
This highlights why bootstrapping remains widely used despite its computational burden. Since it depends on the empirical distribution of the data, it requires fewer theoretical assumptions and produces results that are often more robust. However, bootstrapping also has limitations, such as deciding how many resamples to use and, in the case of parametric bootstrapping, assuming that the sample distribution accurately represents the population. Furthermore, estimating parameters using the mean can make the model vulnerable to outliers, which can distort the results if not accounted properly for.