\subsection{Estimator and estimates}
If we are interested in certain parameters of a population distribution, we can look at a sample. From this, we can make a \textbf{point estimate}. 
\newline
Examples of this are, 
\newline
$\bar{x}$ is a point estimate of $\mu$
\newline
s is a point estimate of $\sigma$
\newline

This is often supplemented with a \textbf{confidence interval}
\newline
This is an interval around the point estimate, where we are confident that the population parameter is located.
\newline

For $\mu$, we have different ways of estimating it. We can use the sample mean $\bar{X}$, or the average $X_T$ of the sample upper and lower quartiles. 
But in this case, we have to look out for \textbf{bias}. If the distribution of a population is skewed, then $X_T$ is biased. The result of this is, that in the long run, this estimator will systematically over or under estimate the value of $\mu$. This is written as,
\newline
$E(X_T) \neq \mu$.
\newline
It is generally preferred that the estimator is \textbf{unbiased}. In this case, $\bar{X}$ is an unbiased estimate of the population mean $\mu$.
\newline

The standard error of $\bar{X}$ is $\frac{\sigma}{\sqrt{n}}$. Here, the standard error decreases, when the sample size increases. If an estimator has this property, it is called \textbf{consistent}. If we compare, the estimator $X_T$ is also consistent, but has a greater variance than $\bar{X}$. 
\newline
It is generally preferred that the estimator has the smallest possible variance, and in that case it is called efficient. So $\bar{X}$ is an efficient estimator.
\newline
When estimating a parameter, the symbol $\hat{}$ is used above it. For $\mu$, $\hat{\mu} = \bar{X}$ .
\newline
We can calculate $\bar{X}$ using the following formula,
$$\bar{X}=\frac{1}{n} \sum_{i=1}^{n}X_i$$   
\newline
For the variance $\sigma$, we can estimate it by using the formula for $S^2$,
$$S^2=\frac{1}{n-1} \sum_{i=1}(X_i-\bar{X})$$
