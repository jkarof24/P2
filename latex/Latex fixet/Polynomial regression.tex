
\section*{Polynomial Regression}
Linear regression is a model that estimates the relationship between a dependent variable, \( y \), and one or more independent variables, \( x \).

A reasonable relationship between the two in simple regression is the linear relationship:

\[
Y = \beta_0 + \beta_1 x
\]

Where \( \beta_0 \) is the intercept, and \( \beta_1 \) is the slope.

In a lot of cases, there will be more independent variables, so the relationship for multiple regression will look like this:

\[
Y = \beta_0 + \beta_1 x_1 + ......+ \beta_n x_n
\]



Where \( n \) is the number of independent variables, and $$\beta_2$$ and further shape the curvature and complexity of the curve. Linear models use the method of least squares of the residuals to estimate parameters, in order to find the best fitting line for the data.

In simple linear regression, the random error \( \epsilon \) is included:

\[
Y = \beta_0 + \beta_1 x + \epsilon
\]

It is assumed that \( \epsilon \) is distributed with $\epsilon = 0$ and $\epsilon) = \sigma^2$, and it has consistent variance, which is usually called the \textit{homogeneous variance assumption}. The random error \( \epsilon \) adds randomness to account for the natural variability in real data, making the model more realistic.
\newline\\
Polynomial regression is a form of linear regression, but the relationship between \( x \) and \( y \) is an \( n \)th-degree polynomial. It fits a nonlinear relationship between the value of \( x \) and the corresponding conditional mean of \( y \), meaning the model predicts the expected value of \( y \) given \( x \).

That is why it is used when the relationship between the independent variable and the dependent variable is better represented by a curve rather than a straight line, since it can show the nonlinear patterns in the data.
In polynomial regression, as mentioned before, there are six assumptions, that always need to be met.


\subsection{The method of least squares}
	To find connections in data, it is necessary to estimate coefficients, $\beta_0$ and $\beta_1$, in linear models. 
A widely used method to estimate the coefficients, is the least squares method that was previously mentioned. 

Least squares considers the deviation of $Y_i$ for  its expected value, where the observations are $(X_i, Y_i)$. 
This method also requires, that we consider the sum of the $n$ squared deviations.
This is denoted by the following function,
$$Q=\sum_{i=1}^{n}(Y_1-\beta_0 - \beta_1 X_i)^2$$

According to the method of least squares, the estimations of $\beta_0$ and $\beta_1$ that minimize Q for the given sample observations $(X_1,Y_1), (X_2,Y_2), ..., (X_n,Y_n)$, are called $b_0$ and $b_1$.  

If an analytical approach is used, the values $b_0$ and $b_1$ that minimize Q for any particular set of sample data are given by these simultaneous equations: 
$$\sum Y_i =n b_0 +b_1 \sum X_i$$
$$\sum X_i Y_i = b_0 \sum X_i + b_1 \sum X_1^2$$

These equations are called $normal equations$, where $b_0$ and $b_1$ are the $point estimators$ of $\beta_0$ and $\beta_1$. It is possible to calculate these normal equations simultaneously for $b_0$ and $b_1$ through these expressions,

$$b_1 = \frac{\sum (X_1 - \bar{X}) (Y_i - \bar{Y})}{\sum (X_i - \bar{X})^2}$$

$$b_0 = \frac{1}{n} (\sum Y_i - b_1 \sum X_i ) = \bar{Y} - b_1 \bar{X}$$

Here, $\bar{X}$ and $\bar{Y}$ are the means of $X_i$ and $Y_i$.

The normal equations can also be derived by differentiating with respect to $\beta_0$ and $\beta_1$:
$$\frac{\partial Q}{\partial \beta_0}=-2 \sum (Y_i - \beta_0 - \beta_1 X_i)$$
$$\frac{\partial Q} {\partial \beta_1} = -2 \sum X_i (Y_i - \beta_0 - \beta_1 X_i)$$

Then $b_0$ and $b_1$ can be set to equal zero, 

$$-2 \sum (Y_i - b_0 - b_1 X_1)=0$$
$$-2\sum X_i(Y_i - b_0 - b_1 X_1)=0$$
This can be simplified, 
$$\sum_{i=1}^{n} (Y_i - b_0 - b_1 Xi)=0£$$
$$\sum_{i=1}^{n} X_1(Y_i - b_0 - b_1 Xi)=0£$$
And it can be expanded, so the normal equations are obtained, 
$$\sum Y_1 - n b_0 - b_1 \sum X_1 =0$$
$$\sum X_1 Y_1  - b_0 \sum X_1 - b_1 \sum X_i^2 =0$$
Solving this will lead to values of $b_0$ and $b_1$, that minimize Q, and these are the estimates for $\beta_0$ and $\beta_1$. 

The estimates $b_0$ and $b_1$ obtain the minimum when checking the second partial derivatives. \newline


Linear models:

Equations for linear models can be written in matrix terms, where the normal error regression model for simple linear regression is.
$$Y_i = \beta_0 + \beta_1 X_i 0 \epsilon_i $$ 


Which implies that,
$$Y_1 = \beta_0 + \beta_1 X_1 + \epsilon_1$$
$$Y_2 = \beta_0 + \beta_2 X_2 + \epsilon_2$$
$$\vdots$$
$$Y_n = \beta_0 + \beta_n X_n + \epsilon_n$$

The observations vector $Y$ is,
$$ Y_{n \times 1} =
\left[
\begin{array}{c}
	Y_1 \\ 
	Y_2 \\ 
	\vdots \\
	Y_n 
\end{array}
\right]
$$	

The X matrix is, 

$$X_{n \times 2}=
\left[
\begin{array}{cc}
	1 & X_1 \\ 
	1 & X_2 \\ 
	\vdots & \vdots \\
	1 & X_n
\end{array}
\right]
$$


The $\beta$ vector is, 
$$ \beta_{2 \times 1} =
\left[
\begin{array}{c}
	\beta_0 \\ 
	\beta_1 
\end{array}
\right]
$$

And the $\epsilon$ vector is,
$$ \epsilon_{n \times 1} =
\left[
\begin{array}{c}
	\epsilon_0 \\ 
	\epsilon_1 \\
	\vdots \\
	\epsilon_n 
\end{array}
\right]
$$

This can be written in matrix terms as, 
$$Y_{n \times 1}=X_{n \times 2} \cdot \beta_{2 \times 1} + \epsilon_{n \times 1}$$	

Where, \newline
\textbf{$Y$} is a vector of response \newline
\textbf{$\beta$} is a vector of parameters \\
\textbf{$X$} is a matrix of constants, called det design matrix\\
\textbf{$\epsilon$} is a vector of independent normal random variables with expectation\\

This can be shown in columns,
$$
\left[
\begin{array}{c}
	Y_1 \\ 
	Y_2 \\ 
	\vdots \\
	Y_n 
\end{array}
\right]
=
\left[
\begin{array}{cc}
	1 & X_1 \\ 
	1 & X_2 \\ 
	\vdots & \vdots \\
	1 & X_n
\end{array}
\right]
\left[
\begin{array}{c}
	\beta_0 \\ 
	\beta_1 
\end{array}
\right]
+
\left[
\begin{array}{c}
	\epsilon_0 \\ 
	\epsilon_1 \\
	\vdots \\
	\epsilon_n 
\end{array}
\right]
$$

If the dependent variable $Y$ has more than one independent variable in a linear model, the equation looks like this, 

$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + ... + \beta_p-1 X_{i, p-1} + \epsilon_i$$

In matrix terms it is,  
$$
\left[
\begin{array}{c}
	Y_1 \\ 
	Y_2 \\ 
	\vdots \\
	Y_n 
\end{array}
\right]
=
\left[
\begin{array}{cccc}
	1 & X_{11} & ... & X_{1, p-1} \\ 
	1 & X_{21} & ... & X_{2, p-1} \\ 
	\vdots & \vdots &  & \vdots \\
	1 & X_{n1} & ... & X_{n, p-1}
\end{array}
\right]
\left[
\begin{array}{c}
	\beta_0 \\ 
	\beta_1 \\
	\vdots \\
	\beta_p-1 
\end{array}
\right]
+
\left[
\begin{array}{c}
	\epsilon_0 \\ 
	\epsilon_1 \\
	\vdots \\
	\epsilon_n 
\end{array}
\right]
$$

The $Y$ and $\epsilon$ vectors are the same as in the simple linear regression matrix. The $\beta$ vector has additional parameters, and the $X$ matrix now has a column of $n$ observations for each $p-1 X$ variables. 


%Skal skrive dette om polynomiel regression i stedet, find et sted at læse om det
For multiple linear regression, the coefficients can still be found through the method of least squares, 

$$Q=\sum_{i=1}^{n}(Y_i -\beta_0 - \beta_1 X_i1 - ... - \beta_{p-1}X_{i,p-1})^2$$

	\subsection{Assumptions}
	\input{Assumptions}