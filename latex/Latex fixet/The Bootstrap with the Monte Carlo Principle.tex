
The following mathematical theory is based on the book, "Mathematical Statistics with Resampling and R" (Chihara \& Hesterberg). The bootstrap is a procedure that uses a given sample to create a new distribution, called the bootstrap distribution. To find the bootstrap distribution, other samples are drawn from the original sample; in that way, it is a tool for resampling. The bootstrap distribution is used to approximate the sample distribution of a specific statistic $\theta$. For most statistics, bootstrap distributions approximate the spread, bias, and shape of the actual sampling distribution. The statistic could for example be the mean. To create the bootstrap distribution of the mean, bootstrap samples are drawn from the original sample, and then the mean is calculated for each resample. In other words, the original sample is now treated as the population. If the statistic tested is the mean, then the bootstrap distribution of the mean will look approximately like the sampling distribution of the mean and have almost the same spread and shape. However, note that the mean of the bootstrap distribution will be the same as the original sample and not the population. \newline
So, the idea of the bootstrap is that the original sample approximates the population. Therefore, resamples of the original sample would approximate the same as taking more samples from the population. That is indeed the concept of resampling.

\subsection{Resampling}
Resampling is a method in statistics used to estimate variability or improve model performance. In simple terms, resampling is creating new samples from existing data. There are different methods for resampling, one of them being bootstrapping. Bootstrapping is resampling with replacement. That means, when new samples are simulated, the same  data point can be chosen multiple times, because when it is selected from the original sample, it is put back before simulating the next sample. That way, the number of observations $n$ is always the same, but some data points might be chosen multiple times and others not at all.

\subsection{Method}
As previously mentioned, the principle of the bootstrap is to use a sample as an approximation of the population. From the samples, bootstrap samples are generated through resampling. The notation $*$ indicates a bootstrap sample. For every bootstrap sample, a statistic noted as $\hat{\theta}^*$ or $\overline{x}^*$ is calculated. The spread of the calculated $\hat{\theta}^*$ is defined as the bootstrap distribution.\newline

Bootstrapping is also used for confidence intervals for a population parameter $\theta$. If the values of the bootstrap distribution are known, it is possible to determine the confidence intervals by calculating the wanted percentiles. Typically a 95\% confidence interval is calculated, which is done by calculating the 2.5\% and 97.5\% percentile. This is called the bootstrap percentile confidence interval.

\subsection{Monte Carlo Principle}
In the bootstrap method with $n$ observations, there would theoretically be $\binom{2n-1}{n}$ different bootstrap samples. For large values of $n$, there would be a large amount of possible bootstrap samples. Therefore, it can be impractical or even impossible to generate all of them. To address this, the Monte Carlo Principle is applied, which means not every bootstrap sample has to be calculated to give precise results. Instead, a random amount of bootstrap samples is generated to approximate the distribution of the statistic of interest. Theory says that for "quick-and-dirty" resampling results 1000 samples should at least be made, but to ensure precise results, 10.000 samples or more are needed.

\subsection{Parametric and nonparametric bootstrap}
There are two types of bootstrapping: parametric and nonparametric. Parametric bootstrapping is when an assumption is made about the population's distribution. For example, assuming the data follows a normal distribution, then the parameters are estimated from that model, and new samples are generated by simulating data from the fitted distribution. Nonparametric bootstrapping is the opposite, when there are no assumptions made about the population's distribution. It resamples from the observed data with replacement and hereby treats the empirical distribution as an estimate for the population. In this project, the focus will be on the nonparametric bootstrap, because that is the one used. \newline

For the nonparametric bootstrap $\hat{F}(x)$ is the empirical cumulative distribution function, given by:
$$\hat{F}(x)=\frac{1}{n}(m\leq x).$$
In addition to this, the nonparametric bootstrap also has an empirical probability mass function $\hat{f}(s)$, given by:
$$\hat{f}(s)=\frac{1}{n}(m=s).$$
In both cases $m$ represents the observed sample, and $n$ is the sample size. Each observation is assigned a probability of $\frac{1}{n}$ to be selected as a bootstrap sample. The  cumulative distribution $\hat{F}(x)$ tells the proportion of values in the sample that are less than or equal to the value $x$. The probability mass function $\hat{f}(s)$ gives the frequency of the value $s$ in the sample. The advantage of this method is that it is based on the sample, without knowing the distribution of the population. No assumptions are made, the data tells what it can, and no bias is introduced based on wrong assumptions.